import os
import asyncio
import traceback
import litellm
from typing import List, Dict, Any, Optional, AsyncGenerator
from enum import Enum, auto

# æ˜¾å¼å¯¼å…¥ç”¨äºç±»å‹æ£€æŸ¥
from litellm import ModelResponse

# å¤ç”¨ä¹‹å‰çš„é…ç½®ç±»ä¸æšä¸¾
class ThinkingConfig:
    def __init__(self, enabled: bool = False, budget: Optional[int] = None):
        self.enabled = enabled
        self.budget = budget

class StreamEventType(Enum):
    REASONING = auto()          # æ€è€ƒè¿‡ç¨‹
    CONTENT = auto()            # æ­£æ–‡å†…å®¹
    TOOL_CALLING_START = auto() # ä¿¡å·ï¼šå¼€å§‹å·¥å…·è°ƒç”¨
    TOOL_CALLING = auto()       # å®Œæ•´å·¥å…·å‚æ•°
    USAGE = auto()              # Token ç”¨é‡
    ERROR = auto()              # é”™è¯¯ä¿¡æ¯

def _prepare_common_args(
    model: str,
    messages: List[Dict[str, Any]],
    api_key: Optional[str],
    base_url: Optional[str],
    temperature: float,
    tools: Optional[List[Dict[str, Any]]],
    parallel_tool_calls: bool,
    thinking_config: Optional[ThinkingConfig],
    **kwargs
) -> Dict[str, Any]:
    """
    é€šç”¨å‚æ•°æ„å»ºé€»è¾‘ï¼ˆåŒæ­¥/å¼‚æ­¥é€šç”¨ï¼‰
    """
    api_kwargs: Dict[str, Any] = {
        "model": model,
        "messages": messages,
        "api_key": api_key or os.environ.get("LLM_API_KEY"),
        "base_url": base_url or os.environ.get("LLM_BASE_URL"),
        "drop_params": True,  # è‡ªåŠ¨ä¸¢å¼ƒä¸æ”¯æŒçš„å‚æ•°
    }

    if tools:
        api_kwargs["tools"] = tools
        api_kwargs["parallel_tool_calls"] = parallel_tool_calls

    if thinking_config and thinking_config.enabled:
        # é’ˆå¯¹ Claude 3.7+ çš„ thinking é€‚é…
        if "claude" in model.lower():
            thinking_kwarg: Dict[str, Any] = {"type": "enabled"}
            if thinking_config.budget:
                thinking_kwarg["budget_tokens"] = thinking_config.budget
            api_kwargs["thinking"] = thinking_kwarg
        else:
            # DeepSeek R1 ç­‰é€šå¸¸é€šè¿‡ reasoning_content è¿”å›ï¼Œæ— éœ€ç‰¹å®šè¯·æ±‚ä½“
            api_kwargs["temperature"] = temperature
    else:
        api_kwargs["temperature"] = temperature

    api_kwargs.update(kwargs)
    return api_kwargs

async def get_async_litellm_response(
    messages: List[Dict[str, Any]], 
    model: str, 
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    temperature: float = 1.0, 
    tools: Optional[List[Dict[str, Any]]] = None,
    parallel_tool_calls: bool = True,
    thinking_config: Optional[ThinkingConfig] = None,
    **kwargs
) -> ModelResponse:
    """
    LiteLLM å¼‚æ­¥éæµå¼è°ƒç”¨ (Async Non-Streaming)
    """
    api_kwargs = _prepare_common_args(
        model, messages, api_key, base_url, temperature, 
        tools, parallel_tool_calls, thinking_config, **kwargs
    )
    api_kwargs["stream"] = False

    try:
        # æ ¸å¿ƒå˜åŒ–ï¼šä½¿ç”¨ acompletion
        response = await litellm.acompletion(**api_kwargs)
        return response # type: ignore
    except Exception:
        # å¼‚æ­¥ç¯å¢ƒä¸‹çš„ Traceback æœ‰æ—¶è¾ƒéš¾è¿½è¸ªï¼Œå»ºè®®åœ¨æ­¤è®°å½•æ—¥å¿—
        raise

async def get_async_stream_litellm_response(
    messages: List[Dict[str, Any]], 
    model: str, 
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    temperature: float = 1.0, 
    tools: Optional[List[Dict[str, Any]]] = None,
    parallel_tool_calls: bool = True,
    thinking_config: Optional[ThinkingConfig] = None,
    **kwargs
) -> AsyncGenerator[tuple[StreamEventType, Any], None]:
    """
    LiteLLM å¼‚æ­¥æµå¼è°ƒç”¨ (Async Streaming)
    """
    api_kwargs = _prepare_common_args(
        model, messages, api_key, base_url, temperature, 
        tools, parallel_tool_calls, thinking_config, **kwargs
    )
    api_kwargs["stream"] = True
    api_kwargs["stream_options"] = {"include_usage": True}

    tool_calls_buffer = {} 
    has_emitted_tool_start = False 

    try:
        # æ ¸å¿ƒå˜åŒ–ï¼šawait acompletion
        stream = await litellm.acompletion(**api_kwargs)
        # æ ¸å¿ƒå˜åŒ–ï¼šasync for è¿­ä»£
        async for chunk in stream: # type: ignore
            
            # 1. Usage å¤„ç†
            if hasattr(chunk, 'usage') and chunk.usage: # type: ignore
                 if chunk.usage.completion_tokens or chunk.usage.prompt_tokens: # type: ignore
                    yield (StreamEventType.USAGE, chunk.usage) # type: ignore
            
            if not chunk.choices:
                continue
                
            delta = chunk.choices[0].delta
            
            # 2. Reasoning (R1 / Claude)
            reasoning = (
                getattr(delta, 'reasoning_content', None) or 
                getattr(delta, 'reasoning', None)
            )
            if reasoning:
                yield (StreamEventType.REASONING, reasoning)

            # 3. Content
            if delta.content:
                yield (StreamEventType.CONTENT, delta.content)
            
            # 4. Tool Calls
            if delta.tool_calls:
                if not has_emitted_tool_start:
                    yield (StreamEventType.TOOL_CALLING_START, None)
                    has_emitted_tool_start = True

                for tool_part in delta.tool_calls:
                    idx = tool_part.index
                    if idx not in tool_calls_buffer:
                        tool_calls_buffer[idx] = {"id": "", "name": "", "args": ""}
                    
                    if tool_part.id:
                        tool_calls_buffer[idx]["id"] = tool_part.id
                    if tool_part.function:
                        if tool_part.function.name:
                            tool_calls_buffer[idx]["name"] = tool_part.function.name
                        if tool_part.function.arguments:
                            tool_calls_buffer[idx]["args"] += tool_part.function.arguments
        
        # 5. Flush Tool Calls
        for idx in sorted(tool_calls_buffer.keys()):
            tool_data = tool_calls_buffer[idx]
            if tool_data["name"]: 
                full_tool_call = {
                    "id": tool_data["id"],
                    "type": "function",
                    "function": {
                        "name": tool_data["name"],
                        "arguments": tool_data["args"]
                    }
                }
                yield (StreamEventType.TOOL_CALLING, full_tool_call)

    except Exception as e:
        traceback.print_exc()
        yield (StreamEventType.ERROR, str(e))


if __name__ == "__main__":
    import dotenv
    from openai import pydantic_function_tool
    from pydantic import BaseModel, Field

    dotenv.load_dotenv(override=True)
    
    API_KEY = os.environ.get("LLM_API_KEY")
    BASE_URL = os.environ.get("LLM_BASE_URL")

    # å®šä¹‰å·¥å…·
    class GetWeather(BaseModel):
        """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”"""
        location: str = Field(description="åŸå¸‚åç§°ï¼Œä¾‹å¦‚ï¼šåŒ—äº¬")

    tool_schemas = [pydantic_function_tool(GetWeather)]

    # å¼‚æ­¥å…¥å£å‡½æ•°
    async def main():
        print(f"ğŸš€ å¼€å§‹å¼‚æ­¥æµ‹è¯• (PID: {os.getpid()})...")
        
        messages = [{"role":"user", "content":"è¯·æ€è€ƒä¸€ä¸‹å¹¶å‘Šè¯‰æˆ‘ï¼ŒåŒ—äº¬ç°åœ¨çš„å¤©æ°”é€‚åˆå‡ºé—¨è·‘æ­¥å—ï¼Ÿ"}]
        
        # è°ƒç”¨å¼‚æ­¥æµå¼å‡½æ•°
        generator = get_async_stream_litellm_response(
            messages=messages,
            model="deepseek/deepseek-reasoner", # éªŒè¯ R1 + Tools
            api_key=API_KEY,
            base_url=BASE_URL, 
            tools=tool_schemas # type: ignore
        )
        
        print("\n--- Stream Start ---")
        async for event_type, data in generator:
            if event_type == StreamEventType.REASONING:
                # æ‰“å°æ€è€ƒè¿‡ç¨‹ (æ·¡è‰²æˆ–ç‰¹å®šæ ‡è®°)
                print(f"\033[90m{data}\033[0m", end="", flush=True)
                
            elif event_type == StreamEventType.CONTENT:
                # æ‰“å°æ­£æ–‡
                print(f"\033[92m{data}\033[0m", end="", flush=True)
                
            elif event_type == StreamEventType.TOOL_CALLING_START:
                print("\n\nğŸ› ï¸  [System] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨æ„å›¾...", flush=True)
                
            elif event_type == StreamEventType.TOOL_CALLING:
                print(f"\nğŸ”§ [Tool] Call: {data['function']['name']} Args: {data['function']['arguments']}")
                
            elif event_type == StreamEventType.USAGE:
                print(f"\n\nğŸ“Š Usage: Prompt({data.prompt_tokens}) + Compl({data.completion_tokens})")
                
        print("\n--- Stream End ---")

    # è¿è¡Œ Event Loop
    asyncio.run(main())
